{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e5b020ca3265da72",
      "metadata": {
        "collapsed": false,
        "id": "e5b020ca3265da72"
      },
      "source": [
        "# Ejercicios UD03_02\n",
        "\n",
        "## Clasificar preguntas\n",
        "\n",
        "En la práctica [Clasificación de texto con PyTorch](https://colab.research.google.com/github/martinezpenya/MIA-IABD-2425/blob/main/UD03/notebooks/2.-classificacio_text_torch_ES.ipynb) hemos visto el proceso para convertir un texto en una representación numérica que pueda ser utilizada por un algoritmo de aprendizaje automático. Hemos visto diferentes representaciones como *Bolsa de palabras* (BoW) y *incrustaciones de palabras* (word embeddings) y cómo entrenar una red neuronal para clasificar texto.\n",
        "\n",
        "En esta práctica, deberá repetir el proceso para clasificar las preguntas en temas. Usaremos el conjunto de datos `Trec` que contiene preguntas en inglés y su tema. El conjunto de datos está disponible en [trec](https://huggingface.co/datasets/CogComp/trec).\n",
        "\n",
        "### Objetivos de la práctica\n",
        "* Reproducir el proceso visto en la práctica [Clasificación de texto con PyTorch](https://colab.research.google.com/github/martinezpenya/MIA-IABD-2425/blob/main/UD03/notebooks/2.-classificacio_text_torch_ES.ipynb) para clasificar preguntas en temáticas.\n",
        "* Deberá preparar una red neuronal con PyTorch para clasificar las preguntas.\n",
        "* Pruebe las diferentes representaciones vistas para convertir el texto en una representación numérica.\n",
        "* Tendrá que comparar los resultados obtenidos con las diferentes representaciones."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalamos las librerías necesarias en las versiones correctas\n",
        "\n",
        "# %pip install --upgrade torch datasets scikit-learn transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Ri9qJSHJ8Z-z"
      },
      "id": "Ri9qJSHJ8Z-z",
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Cargamos el conjunto de datos. Se descargará y almacenará automáticamente en local.\n",
        "# Este conjunto de datos contiene noticias de diferentes categorías. En este caso\n",
        "# usaremos las categorías de mundo, deportes, negocios y ciencia ficción/tecnología.\n",
        "\n",
        "dataset = load_dataset('trec')\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mu_KIPh98fgP",
        "outputId": "1bebe7b3-600d-4dab-a96e-db704202a7d6"
      },
      "id": "Mu_KIPh98fgP",
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'coarse_label', 'fine_label'],\n",
              "        num_rows: 5452\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'coarse_label', 'fine_label'],\n",
              "        num_rows: 500\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset['train'][0])\n",
        "\n",
        "print(dataset['train'].features)\n",
        "\n",
        "classes = dataset['train'].features[\"coarse_label\"].names\n",
        "classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxOfIHBM9yee",
        "outputId": "8139bbbc-de1a-430a-ef13-f8626a6ef974"
      },
      "id": "MxOfIHBM9yee",
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'How did serfdom develop in and then leave Russia ?', 'coarse_label': 2, 'fine_label': 26}\n",
            "{'text': Value(dtype='string', id=None), 'coarse_label': ClassLabel(names=['ABBR', 'ENTY', 'DESC', 'HUM', 'LOC', 'NUM'], id=None), 'fine_label': ClassLabel(names=['ABBR:abb', 'ABBR:exp', 'ENTY:animal', 'ENTY:body', 'ENTY:color', 'ENTY:cremat', 'ENTY:currency', 'ENTY:dismed', 'ENTY:event', 'ENTY:food', 'ENTY:instru', 'ENTY:lang', 'ENTY:letter', 'ENTY:other', 'ENTY:plant', 'ENTY:product', 'ENTY:religion', 'ENTY:sport', 'ENTY:substance', 'ENTY:symbol', 'ENTY:techmeth', 'ENTY:termeq', 'ENTY:veh', 'ENTY:word', 'DESC:def', 'DESC:desc', 'DESC:manner', 'DESC:reason', 'HUM:gr', 'HUM:ind', 'HUM:title', 'HUM:desc', 'LOC:city', 'LOC:country', 'LOC:mount', 'LOC:other', 'LOC:state', 'NUM:code', 'NUM:count', 'NUM:date', 'NUM:dist', 'NUM:money', 'NUM:ord', 'NUM:other', 'NUM:period', 'NUM:perc', 'NUM:speed', 'NUM:temp', 'NUM:volsize', 'NUM:weight'], id=None)}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ABBR', 'ENTY', 'DESC', 'HUM', 'LOC', 'NUM']"
            ]
          },
          "metadata": {},
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Separar el conjunto de datos en entrenamiento y test\n",
        "ds_train = dataset['train']\n",
        "ds_test = dataset['test']\n",
        "# Veamos cuántos ejemplos hay en cada set\n",
        "print('Número de ejemplos de train:', len(ds_train))\n",
        "print('Número de ejemplos de test:', len(ds_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdBDrrpx-H8m",
        "outputId": "33ee3a04-807e-4d2c-beb2-4dbb620b3fb7"
      },
      "id": "LdBDrrpx-H8m",
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de ejemplos de train: 5452\n",
            "Número de ejemplos de test: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimimos los primeros 5 ejemplos del conjunto de entrenamiento\n",
        "for w in ds_train.take(5):\n",
        "    print(f\"{w['coarse_label']} ({classes[w['coarse_label']]}) -> {w['text']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IX8f9udI-Nn9",
        "outputId": "17d5be34-3517-46c9-ee7d-fa1f170e56ac"
      },
      "id": "IX8f9udI-Nn9",
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 (DESC) -> How did serfdom develop in and then leave Russia ?\n",
            "1 (ENTY) -> What films featured the character Popeye Doyle ?\n",
            "2 (DESC) -> How can I find a list of celebrities ' real names ?\n",
            "1 (ENTY) -> What fowl grabs the spotlight after the Chinese Year of the Monkey ?\n",
            "0 (ABBR) -> What is the full form of .com ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Utilizamos el tokenizador de Bert (uno de los primeros modelos de lenguaje basados ​​en transformación) para tokenizar las oraciones\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "\n",
        "# Podiamos ver el vocabulario de tokenización\n",
        "vocab = tokenizer.get_vocab()\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQcK6rf8_MYc",
        "outputId": "8ec0a155-f6e5-42b6-9813-0ab239d1e335"
      },
      "id": "fQcK6rf8_MYc",
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funcion para convertir nuestra cadena tokenizada a números"
      ],
      "metadata": {
        "id": "OhZuBgImJvT8"
      },
      "id": "OhZuBgImJvT8"
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(text):\n",
        "    tk = tokenizer.tokenize(text)\n",
        "    return tokenizer.convert_tokens_to_ids(tk)"
      ],
      "metadata": {
        "id": "Na9QU1su_GLN"
      },
      "id": "Na9QU1su_GLN",
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BoW\n",
        "Funcion para calcular el vector BoW de un comentario del dataset"
      ],
      "metadata": {
        "id": "sSYG4T_DADDs"
      },
      "id": "sSYG4T_DADDs"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "len_vocab = len(vocab)\n",
        "\n",
        "def to_bow(text, tamany_vocabulari=len_vocab):\n",
        "    res = torch.zeros(tamany_vocabulari, dtype=torch.float32)\n",
        "\n",
        "    for i in encode(text):\n",
        "        if i<tamany_vocabulari:\n",
        "            res[i] += 1\n",
        "    return res\n",
        "\n",
        "print(ds_train[0])\n",
        "print(to_bow(ds_train[0][\"text\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17KiIGvo-tbt",
        "outputId": "97b1fd9b-d859-4aa5-e059-f5a3f97ae828"
      },
      "id": "17KiIGvo-tbt",
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'How did serfdom develop in and then leave Russia ?', 'coarse_label': 2, 'fine_label': 26}\n",
            "tensor([0., 0., 0.,  ..., 0., 0., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Función que convierte las palabras textuales en tensores BoW"
      ],
      "metadata": {
        "id": "k5oqiQuQMCdm"
      },
      "id": "k5oqiQuQMCdm"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def bowify(batch):\n",
        "    etiquetas = torch.LongTensor([comentario[\"coarse_label\"] for comentario in batch])\n",
        "    comentarios = torch.stack([to_bow(comentario[\"text\"]) for comentario in batch])\n",
        "\n",
        "    return (\n",
        "            etiquetas,\n",
        "            comentarios\n",
        "    )\n",
        "\n",
        "train_loader = DataLoader(ds_train, batch_size=16, collate_fn=bowify)\n",
        "test_loader = DataLoader(ds_test, batch_size=16, collate_fn=bowify)"
      ],
      "metadata": {
        "id": "icRKFBro_r7F"
      },
      "id": "icRKFBro_r7F",
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Red neuronal\n",
        "El tamaño del vector de entrada es el tamaño del vocabulario, el tamaño de salida corresponde con el número de clases, en este caso 6"
      ],
      "metadata": {
        "id": "B65K6sfiAAH7"
      },
      "id": "B65K6sfiAAH7"
    },
    {
      "cell_type": "code",
      "source": [
        "net = torch.nn.Sequential(\n",
        "    torch.nn.Linear(len(vocab), 6),\n",
        "    torch.nn.LogSoftmax(dim=1)\n",
        ")"
      ],
      "metadata": {
        "id": "AwhGRaD8__ot"
      },
      "id": "AwhGRaD8__ot",
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento del modelo"
      ],
      "metadata": {
        "id": "LW_v-k1MAcUt"
      },
      "id": "LW_v-k1MAcUt"
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(\n",
        "    net,\n",
        "    dataloader,\n",
        "    lr=0.01,\n",
        "    optimizer=None,\n",
        "    loss_fn=torch.nn.NLLLoss(),\n",
        "    epoch_size=None,\n",
        "    report_freq=50\n",
        "):\n",
        "\n",
        "    # Si no se especifica un optimizador, usamos Adam\n",
        "    optimizer = optimizer or torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "    # Ponemos la red en modo de entrenamiento.Esto activa el comportamiento de las capas de DropOut, por ejemplo.\n",
        "    net.train()\n",
        "\n",
        "    # Inicializar las variables que nos servirán para calcular la precisión\n",
        "    total_loss, acc, count, i = 0, 0, 0, 0\n",
        "\n",
        "    # Iteremamos sobre el dataloader\n",
        "    for labels, features in dataloader:\n",
        "\n",
        "        # Ponemos los gradientes a cero\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # calculamos la salida de la red\n",
        "        out = net(features)\n",
        "\n",
        "        # Calculamos la pérdida. Esta función ya se aplica a Softmax a la salida.\n",
        "        loss = loss_fn(out, labels)  # cross_entropy(out,labels)\n",
        "\n",
        "        # Propagamos la pérdida de regreso. Esto hará que se calculen los gradientes .\n",
        "        loss.backward()\n",
        "\n",
        "        # Actualizamos los pesos de la red. Esto toma un paso de optimización.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Actualizamos variables para calcular la precisión.\n",
        "        total_loss += loss\n",
        "\n",
        "        # Calculamos la precisión. Para hacer esto, debemos convertir la salida de red en etiquetas.\n",
        "        # La clase con la mayor probabilidad es la que predecimos como etiqueta.\n",
        "        _, predicted = torch.max(out, 1)\n",
        "        acc += (predicted == labels).sum()\n",
        "\n",
        "        # Actualizamos el contador de muestras\n",
        "        count += len(labels)\n",
        "\n",
        "        # Mostramos la precisión cada report_freq muestras\n",
        "        i += 1\n",
        "        if i % report_freq == 0:\n",
        "            print(f\"{count}: acc={acc.item()/count}\")\n",
        "\n",
        "        # Si se especifica epoch_size y ya hemos procesado este número de muestras, dejamos el bucle.\n",
        "        if epoch_size and count > epoch_size:\n",
        "            break\n",
        "    return total_loss.item() / count, acc.item() / count\n",
        "\n",
        "\n",
        "train_epoch(net, train_loader, epoch_size=5452)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8DwUk-aAbFM",
        "outputId": "32e09805-1eb7-4205-d2bd-97c85d0635f3"
      },
      "id": "o8DwUk-aAbFM",
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "800: acc=0.4725\n",
            "1600: acc=0.5575\n",
            "2400: acc=0.6033333333333334\n",
            "3200: acc=0.6375\n",
            "4000: acc=0.665\n",
            "4800: acc=0.68375\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.059885379066383325, 0.6975421863536317)"
            ]
          },
          "metadata": {},
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Words2Vec"
      ],
      "metadata": {
        "id": "SD9oN4uEDY9U"
      },
      "id": "SD9oN4uEDY9U"
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "w2v = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "id": "rtVe7f28DV6B"
      },
      "id": "rtVe7f28DV6B",
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clasificación Word2Vec\n",
        "Función que recibe un texto y devuelve un vector con la representación w2v del texto"
      ],
      "metadata": {
        "id": "D7FcQke7D_eZ"
      },
      "id": "D7FcQke7D_eZ"
    },
    {
      "cell_type": "code",
      "source": [
        "def to_w2v(text):\n",
        "    res = torch.zeros(300, dtype=torch.float32)\n",
        "    for word in text:\n",
        "        if word in w2v:\n",
        "            res += torch.tensor(w2v.get_vector(word))\n",
        "    return res\n",
        "\n",
        "print(to_w2v(ds_train[0][\"text\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dqz5yvWsD5tg",
        "outputId": "5a0e6f93-eb08-4944-d7bb-b669a2b1f4a5"
      },
      "id": "Dqz5yvWsD5tg",
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-4.8299e+00,  4.6221e+00,  6.3916e-01,  4.6835e+00, -1.8214e+00,\n",
            "         1.2753e+00, -3.7478e+00, -1.9570e+00, -2.1226e+00,  1.1957e+00,\n",
            "        -1.4957e+00, -3.6926e+00, -6.9828e+00,  1.2891e+00, -5.4214e+00,\n",
            "         2.3289e+00,  6.3632e+00,  6.8435e+00, -6.9910e-01,  1.6830e-01,\n",
            "        -9.7990e+00, -1.6850e+00,  4.2969e+00,  1.0471e+00, -3.7348e+00,\n",
            "         1.4172e-03, -8.9905e+00,  1.9579e+00, -1.5481e+00, -7.7084e-01,\n",
            "        -1.9058e+00,  1.5775e+00, -4.2176e+00, -4.4065e+00, -4.8470e+00,\n",
            "         2.6566e+00, -6.9663e+00,  5.5089e+00, -3.0235e+00,  3.2943e+00,\n",
            "        -8.1671e-01, -7.1863e-01,  6.5320e-01,  2.8318e+00,  2.1558e+00,\n",
            "        -2.7233e+00, -1.3381e+00, -8.1541e+00, -5.0592e+00,  2.5273e+00,\n",
            "        -7.7108e+00,  9.1387e+00, -5.1489e-01,  7.0222e+00,  1.3050e+00,\n",
            "         5.3617e+00, -6.4973e+00, -3.7470e+00,  1.9152e+00, -6.4446e+00,\n",
            "        -5.6579e+00, -4.1564e+00, -7.2266e+00, -1.7032e+00, -2.4363e+00,\n",
            "        -7.6045e+00, -3.9795e+00,  4.5981e+00, -1.1135e+00,  2.0242e+00,\n",
            "         1.6733e+00, -2.3152e+00,  1.8364e+00, -6.5504e-01, -4.6990e-01,\n",
            "        -7.9041e-01,  4.9297e+00,  4.3292e-01,  8.7250e-01, -3.1550e+00,\n",
            "        -5.6450e+00, -7.4365e-01, -3.4029e-01, -3.9285e-01,  4.4510e+00,\n",
            "         2.4427e+00, -1.9684e+00,  9.9484e+00,  8.7151e-01,  3.9846e+00,\n",
            "         1.8136e+00, -5.6177e-01, -4.4843e-01, -3.2343e+00,  1.1763e+00,\n",
            "         3.9058e+00, -3.4902e+00,  4.9506e+00,  8.8367e+00, -1.6271e+00,\n",
            "        -1.9514e+00,  6.0437e-01, -1.8094e+00, -2.6257e-01, -2.2544e+00,\n",
            "         4.8032e+00, -2.8088e+00,  1.5581e+00, -7.3547e-01,  7.7841e-01,\n",
            "        -4.0171e+00, -6.7764e+00, -2.1484e+00, -1.6216e+00,  2.2803e+00,\n",
            "         5.9974e+00,  3.2974e+00,  3.2275e-01, -4.2783e-01, -1.6368e+00,\n",
            "         5.1778e+00,  3.6018e+00,  6.9281e-01, -3.8550e-01,  3.2273e+00,\n",
            "        -6.4236e+00, -4.7470e+00, -1.0410e+00,  1.6462e+00,  4.8635e+00,\n",
            "        -2.7832e+00, -1.8018e+00, -3.4323e+00, -2.5291e+00, -6.0046e+00,\n",
            "         3.3912e+00, -1.4084e+00,  1.9287e+00,  7.1552e+00,  6.0167e+00,\n",
            "         7.5708e+00, -1.5102e+00,  2.7758e+00, -2.4504e+00, -1.8004e+00,\n",
            "        -4.2364e+00, -2.6421e+00,  5.5850e+00, -2.1781e+00,  1.1656e+00,\n",
            "         3.3915e+00, -9.7124e+00, -1.5088e+00, -2.6342e+00, -4.4884e+00,\n",
            "        -2.4268e+00,  2.6746e+00,  5.8071e+00, -4.1341e-01, -1.1578e+00,\n",
            "         4.5178e-01,  2.7651e+00, -8.2956e-01,  1.3205e-01, -4.2538e-01,\n",
            "        -7.6584e-01,  6.5460e+00,  7.3561e-01, -4.4885e+00,  2.8083e+00,\n",
            "        -3.6256e+00, -3.8717e+00, -7.2363e-01, -2.4452e+00, -7.5154e-01,\n",
            "         9.3360e+00,  1.0414e+01, -6.5034e+00,  3.3691e-01, -2.5573e+00,\n",
            "        -2.3767e-01,  1.5276e-01,  1.8858e+00, -3.1852e+00, -1.5726e+00,\n",
            "         3.1674e+00, -1.0554e+00,  2.6754e+00, -1.2245e+00,  5.3683e-01,\n",
            "         3.8909e+00, -2.3540e+00, -9.2539e+00, -1.2234e-01,  5.0328e+00,\n",
            "         1.9660e+00, -2.1343e+00, -1.1998e+00,  2.6303e+00, -5.4028e+00,\n",
            "        -3.0837e+00,  1.6240e+00, -6.0752e+00, -5.2529e+00,  1.5087e+00,\n",
            "        -3.1180e+00, -3.6365e-01,  4.1628e+00, -1.2565e+00,  5.7229e+00,\n",
            "         6.0436e+00,  3.2227e+00, -4.3240e+00,  1.2970e+00, -2.1122e+00,\n",
            "         4.2285e-01,  6.1846e+00, -3.1523e+00, -5.8741e+00, -1.0057e+00,\n",
            "        -4.3607e+00,  4.8915e+00,  2.5996e+00, -3.2574e-01, -1.4861e+00,\n",
            "        -3.7690e+00,  2.3404e+00,  2.0685e+00,  1.4141e+00, -4.3155e+00,\n",
            "         1.5330e+00, -1.4219e+00, -3.0078e+00, -6.8640e-01,  8.4995e-01,\n",
            "         6.0134e-01,  1.3878e+00,  4.1549e+00,  3.4801e+00,  4.4655e+00,\n",
            "         3.8021e+00,  2.5654e+00,  1.0211e+01, -2.0497e+00, -1.2334e+00,\n",
            "         2.5285e+00, -5.9279e+00,  5.3393e+00,  1.3952e+00, -9.9644e-01,\n",
            "         1.3015e+00,  2.5898e+00,  2.5699e+00,  4.4753e+00, -1.1338e+00,\n",
            "        -3.8366e+00, -1.8562e+00,  3.4169e+00,  1.0506e+00, -3.7098e+00,\n",
            "         2.3228e+00, -5.2581e-01,  4.9185e+00, -1.1687e+00, -2.8671e+00,\n",
            "         2.3771e+00,  9.8511e-02,  1.2390e+00,  1.0245e-01, -5.8096e-01,\n",
            "        -3.2206e+00, -6.1292e-01, -1.8250e-01,  6.0913e-01,  2.1962e+00,\n",
            "        -2.4573e+00, -6.8301e+00, -5.4893e+00, -2.9685e+00,  3.1358e+00,\n",
            "        -3.5565e+00,  4.8179e+00, -5.9076e-01, -1.2000e+00, -1.4375e-01,\n",
            "         2.6431e+00,  4.9927e-02,  1.5433e+00, -1.2435e+00, -1.6801e+00,\n",
            "         2.7289e+00, -1.4534e+00, -4.6512e+00,  4.8253e+00, -1.0173e+00,\n",
            "        -5.8792e+00, -4.3466e+00, -6.4514e-01, -4.5042e+00,  5.3074e+00])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Función que convierte los datos textales en tensores w2v"
      ],
      "metadata": {
        "id": "e_DEU5dyN5jW"
      },
      "id": "e_DEU5dyN5jW"
    },
    {
      "cell_type": "code",
      "source": [
        "def w2vify(batch):\n",
        "    etiquetes = torch.LongTensor([comentario[\"coarse_label\"] for comentario in batch])\n",
        "    comentarios = torch.stack([to_w2v(tokenizer.tokenize(comentario[\"text\"])) for comentario in batch])\n",
        "    return etiquetes, comentarios\n",
        "\n",
        "train_loader = DataLoader(ds_train, batch_size=16, collate_fn=w2vify)\n",
        "test_loader = DataLoader(ds_test, batch_size=16, collate_fn=w2vify)"
      ],
      "metadata": {
        "id": "u068n0MdEWSb"
      },
      "id": "u068n0MdEWSb",
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = torch.nn.Sequential(\n",
        "    torch.nn.Linear(300, 6),\n",
        "    torch.nn.LogSoftmax(dim=1)\n",
        ")"
      ],
      "metadata": {
        "id": "eu8pMz4OEqix"
      },
      "id": "eu8pMz4OEqix",
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_epoch(net, train_loader, epoch_size=5452)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGtHcHsFFWwy",
        "outputId": "3f9668c8-d084-4f99-dfd2-ce91d06f0445"
      },
      "id": "NGtHcHsFFWwy",
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "800: acc=0.51\n",
            "1600: acc=0.580625\n",
            "2400: acc=0.6025\n",
            "3200: acc=0.6221875\n",
            "4000: acc=0.64075\n",
            "4800: acc=0.6504166666666666\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.05915870022791235, 0.6555392516507703)"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como podemos observar los resultados son ligeramente mejores con BoW que con Words2Vec"
      ],
      "metadata": {
        "id": "KLuvkA9uKHzI"
      },
      "id": "KLuvkA9uKHzI"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}