{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9b37365737140463",
      "metadata": {
        "collapsed": false,
        "id": "9b37365737140463"
      },
      "source": [
        "# Creación de un prototipo de asistente virtual\n",
        "\n",
        "Esta práctica propone la creación de un prototipo del sistema de traducción que permite recibir órdenes en un idioma y ejecutarlos en otro idioma. Por ejemplo, el usuario podría decir \"open the door\" y el sistema respondería \"abre la puerta\". Este prototipo se basará en el uso de modelos de HugginFace previamente entrenados para la traducción y la síntesis de voz."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9936077d0223c66f",
      "metadata": {
        "collapsed": false,
        "id": "9936077d0223c66f"
      },
      "source": [
        "## Buscar modelos previamente entrenados\n",
        "\n",
        "El primer paso es buscar modelos previamente entrenados para las tareas solicitadas. Investigue Huggingface para seleccionar modelos que mejor satisfagan las necesidades del prototipo.\n",
        "\n",
        "Cuando haya seleccionado los modelos, modifique las siguientes variables para incluirlas en el prototipo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "c82d304a41b86f34",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-02-12T12:12:15.937726Z",
          "start_time": "2024-02-12T12:12:15.876568Z"
        },
        "id": "c82d304a41b86f34"
      },
      "outputs": [],
      "source": [
        "# Modelo de voz de texto\n",
        "MODELO_VOZ_A_TEXTO = \"openai/whisper-small\"\n",
        "\n",
        "# Model de traducción ingles a español\n",
        "MODELO_TRADUCCION = \"Helsinki-NLP/opus-mt-en-es\"\n",
        "\n",
        "# Modelo de texto a voz\n",
        "MODELO_TEXTO_A_VOZ = \"suno/bark\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "243cf421240069fa",
      "metadata": {
        "collapsed": false,
        "id": "243cf421240069fa"
      },
      "source": [
        "## Implementación del prototipo\n",
        "\n",
        "Una vez que se seleccionan los modelos previamente entrenados, implementa el prototipo.Para hacer esto, puede seguir los siguientes pasos:\n",
        "\n",
        "1. Cree una función que, dada una orden de voz, lo transforme en texto (_Automatic Speech Recognition_ ASR). Para hacer esto, puede usar el modelo `MODELO_VOZ_A_TEXTO` previamente seleccionado. Como ejemplo, puede usar el siguiente archivo de voz: [OpenTheDoor.wav](OpenTheDoor.wav)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "338ba3514989899f",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-02-12T12:12:15.938435Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "338ba3514989899f",
        "is_executing": true,
        "outputId": "0e77aa0b-8d60-4681-80cb-0c880abede01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "  Open the door.\n"
          ]
        }
      ],
      "source": [
        "# Creamos la tubería para la conversión de voz a texto\n",
        "from transformers import pipeline\n",
        "\n",
        "asr_pipeline = pipeline(task=\"automatic-speech-recognition\", model=MODELO_VOZ_A_TEXTO)\n",
        "\n",
        "# Convertimos la voz en texto\n",
        "def voz_a_texto(archivo_audio):\n",
        "    # Convertimos la voz en texto\n",
        "    resultado = asr_pipeline(archivo_audio)\n",
        "    texto_transcrito = resultado['text']\n",
        "    return texto_transcrito\n",
        "\n",
        "texto = voz_a_texto(\"OpenTheDoor.wav\")\n",
        "print(\"\\n\\n\",texto)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8af0a933321b4a91",
      "metadata": {
        "collapsed": false,
        "id": "8af0a933321b4a91"
      },
      "source": [
        "2. Crea una función que, dado un texto y un idioma, lo traduce al español (_machine translation_). Para hacer esto, puede usar el modelo `MODELO_TRADUCCION` seleccionado anteriormente. Como ejemplo, puede usar el siguiente pedido en texto: \"Abra la puerta\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "1bd3c8ff42a90ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bd3c8ff42a90ec",
        "is_executing": true,
        "outputId": "ee406e1c-a508-496d-a97a-ea7f454033b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " Abre la puerta.\n"
          ]
        }
      ],
      "source": [
        "# Creamos la tubería para la traducción\n",
        "translation_pipeline = pipeline(task=\"translation\", model=MODELO_TRADUCCION)\n",
        "\n",
        "# Traducimos el texto al español\n",
        "def traducir_texto(texto):\n",
        "    # Traducimos el texto al español\n",
        "    resultado = translation_pipeline(texto)\n",
        "    texto_traducido = resultado[0]['translation_text']\n",
        "    return texto_traducido\n",
        "\n",
        "texto_traducido = traducir_texto(texto)\n",
        "print(\"\\n\\n\",texto_traducido)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7508ebc5f4f49b64",
      "metadata": {
        "collapsed": false,
        "id": "7508ebc5f4f49b64"
      },
      "source": [
        "3. Crea una funció que, donat un text, el sintetitzi en veu (_Text to speech_). Per fer-ho pots utilitzar el model `MODEL_TEXT_A_VEU` seleccionat anteriorment. Com a exemple pots utilitzar el text \"abre la puerta\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "78e6d98ccacc786c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550,
          "referenced_widgets": [
            "b68943c12a844bd5ac596a5bf8b91f37",
            "61d64763ca274ca08ac0882460a1bbcc",
            "daf394158e9f4bd790dea8522422b328",
            "918445d3527543cd86c46ef1e165f586",
            "9ce5e344fd4a459da06040cf22a2cf12",
            "063d5131233e4f3e9a837b480d230beb",
            "33eca2dcf66348d09fd64f07918cf395",
            "f6f061b0b652408391ab621e409cb6dc",
            "a394f5c4157c4265955ff006bd9b9e9f",
            "0c2199a7c83f47d8911e97895f6304e3",
            "8ee6f2a54ca34b339e4a4a83c01b8f14"
          ]
        },
        "id": "78e6d98ccacc786c",
        "is_executing": true,
        "outputId": "50ec0a7a-caa5-4466-dd5b-ed030523773b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/4.37k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b68943c12a844bd5ac596a5bf8b91f37"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unrecognized model in coqui/XTTS-v2. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deformable_detr, deit, depth_anything, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, git, glm, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, graphormer, grounding-dino, groupvit, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rwkv, sam, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, siglip, siglip_vision_model, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zoedepth",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-c65d22314507>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Creamos la tubería para la síntesis de voz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtts_pipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text-to-speech\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODELO_TEXTO_A_VOZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Sintetizamos el texto en voz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtexto_a_voz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    847\u001b[0m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"base_model_name_or_path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m         config = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    850\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_from_pipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode_revision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcode_revision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1089\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1092\u001b[0m             \u001b[0;34mf\"Unrecognized model in {pretrained_model_name_or_path}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[0;34mf\"Should have a `model_type` key in its {CONFIG_NAME}, or contain one of the following strings \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unrecognized model in coqui/XTTS-v2. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deformable_detr, deit, depth_anything, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, git, glm, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, graphormer, grounding-dino, groupvit, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llava, llava_n..."
          ]
        }
      ],
      "source": [
        "# Creamos la tubería para la síntesis de voz\n",
        "tts_pipeline = pipeline(task=\"text-to-speech\", model=MODELO_TEXTO_A_VOZ)\n",
        "\n",
        "# Sintetizamos el texto en voz\n",
        "def texto_a_voz(texto):\n",
        "    # Sintetizamos el texto en voz\n",
        "    out = tts_pipeline(texto)\n",
        "    return out\n",
        "\n",
        "out = texto_a_voz(texto_traducido)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ng0-gZ-oRjY2",
      "metadata": {
        "id": "ng0-gZ-oRjY2"
      },
      "outputs": [],
      "source": [
        "# mostramos el texto sintetizado como un reproductor\n",
        "from IPython.display import Audio\n",
        "\n",
        "Audio(out[\"audio\"], rate=out['sampling_rate'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gzPwaMiHcHiS",
      "metadata": {
        "id": "gzPwaMiHcHiS"
      },
      "source": [
        "4.- Une las tres funciones anteriores en una sola función que, dada una voz y un idioma, lo transforma en texto, lo traduce en otro idioma y la sintetizando en la voz. Como ejemplo, puede usar el siguiente audio de voz: [OpenTheDoor.wav](OpenTheDoor.wav)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RM2xBgS4cswx",
      "metadata": {
        "id": "RM2xBgS4cswx"
      },
      "outputs": [],
      "source": [
        "# Aquí generamos una funcion que lo aglutine todo\n",
        "from IPython.display import Audio\n",
        "\n",
        "def assistant(voice_order):\n",
        "    # Convertim la veu a text\n",
        "    text = asr_pipeline(voice_order)['text']\n",
        "    print(\"Text from audio:\", text)\n",
        "\n",
        "    # Traduïm el text al castellà\n",
        "    text_translated = translation_pipeline(text)[0]['translation_text']\n",
        "    print(\"Text in spanish:\", text_translated)\n",
        "\n",
        "    # Sintetitzem el text en veu\n",
        "    out = tts_pipeline(text_translated)\n",
        "\n",
        "    return out\n",
        "\n",
        "out = assistant(\"OpenTheDoor.wav\")\n",
        "\n",
        "Audio(out[\"audio\"], rate=out['sampling_rate'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b68943c12a844bd5ac596a5bf8b91f37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_61d64763ca274ca08ac0882460a1bbcc",
              "IPY_MODEL_daf394158e9f4bd790dea8522422b328",
              "IPY_MODEL_918445d3527543cd86c46ef1e165f586"
            ],
            "layout": "IPY_MODEL_9ce5e344fd4a459da06040cf22a2cf12"
          }
        },
        "61d64763ca274ca08ac0882460a1bbcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_063d5131233e4f3e9a837b480d230beb",
            "placeholder": "​",
            "style": "IPY_MODEL_33eca2dcf66348d09fd64f07918cf395",
            "value": "config.json: 100%"
          }
        },
        "daf394158e9f4bd790dea8522422b328": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6f061b0b652408391ab621e409cb6dc",
            "max": 4368,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a394f5c4157c4265955ff006bd9b9e9f",
            "value": 4368
          }
        },
        "918445d3527543cd86c46ef1e165f586": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c2199a7c83f47d8911e97895f6304e3",
            "placeholder": "​",
            "style": "IPY_MODEL_8ee6f2a54ca34b339e4a4a83c01b8f14",
            "value": " 4.37k/4.37k [00:00&lt;00:00, 144kB/s]"
          }
        },
        "9ce5e344fd4a459da06040cf22a2cf12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "063d5131233e4f3e9a837b480d230beb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33eca2dcf66348d09fd64f07918cf395": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6f061b0b652408391ab621e409cb6dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a394f5c4157c4265955ff006bd9b9e9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0c2199a7c83f47d8911e97895f6304e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ee6f2a54ca34b339e4a4a83c01b8f14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}