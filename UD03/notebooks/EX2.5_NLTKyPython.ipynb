{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "afcea688-88b6-46ea-a926-e12770c2aaa5",
      "metadata": {
        "id": "afcea688-88b6-46ea-a926-e12770c2aaa5"
      },
      "source": [
        "# Ejercicios UD03_02.5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a2f1713",
      "metadata": {
        "id": "9a2f1713"
      },
      "source": [
        "## Uso de NLTK y Python\n",
        "\n",
        "* *Referencias:*\n",
        "    * *http://www.nltk.org/*\n",
        "    * *http://www.python.org/*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bd2f7bd-9dbf-4e6b-a44c-b28e50be76eb",
      "metadata": {
        "id": "7bd2f7bd-9dbf-4e6b-a44c-b28e50be76eb"
      },
      "source": [
        "### a)Procesamiento del corpus cess_esp anotado con información morfosintáctica.\n",
        "\n",
        "* Dividir el corpus en dos partes: training (el 90% de las primeras frases) y de test (el 10% restante)\n",
        "* Reducir el conjunto de etiquetas morfosintácticas del corpus (289) a un conjunto reducido(67). Para ello, considerar las etiquetas de longitud =2 salvo los verbos (v) y los signos de puntuación (F) que pueden ser de tres. También pueden existir etiquetas de longitud =1. Eliminar también las tuplas vacias, por ejemplo: (u'*0*', u'sn'), son tuplas que el primer elemento tiene el valor '*0*'.\n",
        "\n",
        ">Nota: para entender el significado de las etiquetas se puede consultar el siguiente enlace: https://freeling-user-manual.readthedocs.io/en/latest/tagsets/tagset-es/\n",
        "\n",
        "Ejemplo de lo que se pide:\n",
        "* original\n",
        "```\n",
        "[[('El', 'da0ms0'), ('grupo', 'ncms000'), ('estatal', 'aq0cs0'), ('Electricité_de_France', 'np00000'), ('-Fpa-', 'Fpa'), ('EDF', 'np00000'), ('-Fpt-', 'Fpt'), ('anunció', 'vmis3s0'), ('hoy', 'rg'), (',', 'Fc'), ('jueves', 'W'), (',', 'Fc'), ('la', 'da0fs0'), ('compra', 'ncfs000'), ('del', 'spcms'), ('51_por_ciento', 'Zp'), ('de', 'sps00'), ('la', 'da0fs0'), ('empresa', 'ncfs000'), ('mexicana', 'aq0fs0'), ('Electricidad_Águila_de_Altamira', 'np00000'), ('-Fpa-', 'Fpa'), ('EAA', 'np00000'), ('-Fpt-', 'Fpt'), (',', 'Fc'), ('creada', 'aq0fsp'), ('por', 'sps00'), ('el', 'da0ms0'), ('japonés', 'aq0ms0'), ('Mitsubishi_Corporation', 'np00000'), ('para', 'sps00'), ('poner_en_marcha', 'vmn0000'), ('una', 'di0fs0'), ('central', 'ncfs000'), ('de', 'sps00'), ('gas', 'ncms000'), ('de', 'sps00'), ('495', 'Z'), ('megavatios', 'ncmp000'), ('.', 'Fp')], [('Una', 'di0fs0'), ('portavoz', 'nccs000'), ('de', 'sps00'), ('EDF', 'np00000'), ('explicó', 'vmis3s0'), ('a', 'sps00'), ('EFE', 'np00000'), ('que', 'cs'), ('el', 'da0ms0'), ('proyecto', 'ncms000'), ('para', 'sps00'), ('la', 'da0fs0'), ('construcción', 'ncfs000'), ('de', 'sps00'), ('Altamira_2', 'np00000'), (',', 'Fc'), ('al', 'spcms'), ('norte', 'ncms000'), ('de', 'sps00'), ('Tampico', 'np00000'), (',', 'Fc'), ('prevé', 'vmm02s0'), ('la', 'da0fs0'), ('utilización', 'ncfs000'), ('de', 'sps00'), ('gas', 'ncms000'), ('natural', 'aq0cs0'), ('como', 'cs'), ('combustible', 'ncms000'), ('principal', 'aq0cs0'), ('en', 'sps00'), ('una', 'di0fs0'), ('central', 'ncfs000'), ('de', 'sps00'), ('ciclo', 'ncms000'), ('combinado', 'aq0msp'), ('que', 'pr0cn000'), ('debe', 'vmip3s0'), ('empezar', 'vmn0000'), ('a', 'sps00'), ('funcionar', 'vmn0000'), ('en', 'sps00'), ('mayo_del_2002', 'W'), ('.', 'Fp')]]\n",
        "```\n",
        "* nuevo\n",
        "```\n",
        "[[('El', 'da'), ('grupo', 'nc'), ('estatal', 'aq'), ('Electricité_de_France', 'np'), ('-Fpa-', 'fpa'), ('EDF', 'np'), ('-Fpt-', 'fpt'), ('anunció', 'vmi'), ('hoy', 'rg'), (',', 'fc'), ('jueves', 'w'), (',', 'fc'), ('la', 'da'), ('compra', 'nc'), ('del', 'sp'), ('51_por_ciento', 'zp'), ('de', 'sp'), ('la', 'da'), ('empresa', 'nc'), ('mexicana', 'aq'), ('Electricidad_Águila_de_Altamira', 'np'), ('-Fpa-', 'fpa'), ('EAA', 'np'), ('-Fpt-', 'fpt'), (',', 'fc'), ('creada', 'aq'), ('por', 'sp'), ('el', 'da'), ('japonés', 'aq'), ('Mitsubishi_Corporation', 'np'), ('para', 'sp'), ('poner_en_marcha', 'vmn'), ('una', 'di'), ('central', 'nc'), ('de', 'sp'), ('gas', 'nc'), ('de', 'sp'), ('495', 'z'), ('megavatios', 'nc'), ('.', 'fp')], [('Una', 'di'), ('portavoz', 'nc'), ('de', 'sp'), ('EDF', 'np'), ('explicó', 'vmi'), ('a', 'sp'), ('EFE', 'np'), ('que', 'cs'), ('el', 'da'), ('proyecto', 'nc'), ('para', 'sp'), ('la', 'da'), ('construcción', 'nc'), ('de', 'sp'), ('Altamira_2', 'np'), (',', 'fc'), ('al', 'sp'), ('norte', 'nc'), ('de', 'sp'), ('Tampico', 'np'), (',', 'fc'), ('prevé', 'vmm'), ('la', 'da'), ('utilización', 'nc'), ('de', 'sp'), ('gas', 'nc'), ('natural', 'aq'), ('como', 'cs'), ('combustible', 'nc'), ('principal', 'aq'), ('en', 'sp'), ('una', 'di'), ('central', 'nc'), ('de', 'sp'), ('ciclo', 'nc'), ('combinado', 'aq'), ('que', 'pr'), ('debe', 'vmi'), ('empezar', 'vmn'), ('a', 'sp'), ('funcionar', 'vmn'), ('en', 'sp'), ('mayo_del_2002', 'w'), ('.', 'fp')]]\n",
        "```\n",
        "* opcional:\n",
        "    * número de frases: 6030\n",
        "    * número de palabras: 192686\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6bf51799-c2de-4f4d-b8bb-95dbd6ea2ebb",
      "metadata": {
        "id": "6bf51799-c2de-4f4d-b8bb-95dbd6ea2ebb"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1958cdd1",
      "metadata": {
        "id": "1958cdd1",
        "outputId": "bde328fb-b52c-482b-ac95-e7df59e91349",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> cess_esp\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "    Downloading package cess_esp to /root/nltk_data...\n",
            "      Unzipping corpora/cess_esp.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "#Descargar el corpus\n",
        "#1. ejecutar esta linea\n",
        "nltk.download()\n",
        "#2. elegir la opción d) Download\n",
        "#3. escribir identificador del corpus que deseamos, en nuestro caso 'cess_esp', 'punkt'... o bien 'all'\n",
        "#4. q) quit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fe24a7cb",
      "metadata": {
        "id": "fe24a7cb",
        "outputId": "2c1a3106-2bb3-4e4f-ba66-216d15649f75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Primeras 3 frases del corpus original:\n",
            "[('El', 'da0ms0'), ('grupo', 'ncms000'), ('estatal', 'aq0cs0'), ('Electricité_de_France', 'np00000'), ('-Fpa-', 'Fpa'), ('EDF', 'np00000'), ('-Fpt-', 'Fpt'), ('anunció', 'vmis3s0'), ('hoy', 'rg'), (',', 'Fc'), ('jueves', 'W'), (',', 'Fc'), ('la', 'da0fs0'), ('compra', 'ncfs000'), ('del', 'spcms'), ('51_por_ciento', 'Zp'), ('de', 'sps00'), ('la', 'da0fs0'), ('empresa', 'ncfs000'), ('mexicana', 'aq0fs0'), ('Electricidad_Águila_de_Altamira', 'np00000'), ('-Fpa-', 'Fpa'), ('EAA', 'np00000'), ('-Fpt-', 'Fpt'), (',', 'Fc'), ('creada', 'aq0fsp'), ('por', 'sps00'), ('el', 'da0ms0'), ('japonés', 'aq0ms0'), ('Mitsubishi_Corporation', 'np00000'), ('para', 'sps00'), ('poner_en_marcha', 'vmn0000'), ('una', 'di0fs0'), ('central', 'ncfs000'), ('de', 'sps00'), ('gas', 'ncms000'), ('de', 'sps00'), ('495', 'Z'), ('megavatios', 'ncmp000'), ('.', 'Fp')]\n",
            "[('Una', 'di0fs0'), ('portavoz', 'nccs000'), ('de', 'sps00'), ('EDF', 'np00000'), ('explicó', 'vmis3s0'), ('a', 'sps00'), ('EFE', 'np00000'), ('que', 'cs'), ('el', 'da0ms0'), ('proyecto', 'ncms000'), ('para', 'sps00'), ('la', 'da0fs0'), ('construcción', 'ncfs000'), ('de', 'sps00'), ('Altamira_2', 'np00000'), (',', 'Fc'), ('al', 'spcms'), ('norte', 'ncms000'), ('de', 'sps00'), ('Tampico', 'np00000'), (',', 'Fc'), ('prevé', 'vmm02s0'), ('la', 'da0fs0'), ('utilización', 'ncfs000'), ('de', 'sps00'), ('gas', 'ncms000'), ('natural', 'aq0cs0'), ('como', 'cs'), ('combustible', 'ncms000'), ('principal', 'aq0cs0'), ('en', 'sps00'), ('una', 'di0fs0'), ('central', 'ncfs000'), ('de', 'sps00'), ('ciclo', 'ncms000'), ('combinado', 'aq0msp'), ('que', 'pr0cn000'), ('debe', 'vmip3s0'), ('empezar', 'vmn0000'), ('a', 'sps00'), ('funcionar', 'vmn0000'), ('en', 'sps00'), ('mayo_del_2002', 'W'), ('.', 'Fp')]\n",
            "[('La', 'da0fs0'), ('electricidad', 'ncfs000'), ('producida', 'aq0fsp'), ('pasará', 'vmif3s0'), ('a', 'sps00'), ('la', 'da0fs0'), ('red', 'ncfs000'), ('eléctrica', 'aq0fs0'), ('pública', 'aq0fs0'), ('de', 'sps00'), ('México', 'np00000'), ('en_virtud_de', 'sps00'), ('un', 'di0ms0'), ('acuerdo', 'ncms000'), ('de', 'sps00'), ('venta', 'ncfs000'), ('de', 'sps00'), ('energía', 'ncfs000'), ('de', 'sps00'), ('EAA', 'np00000'), ('con', 'sps00'), ('la', 'da0fs0'), ('Comisión_Federal_de_Electricidad', 'np00000'), ('-Fpa-', 'Fpa'), ('CFE', 'np00000'), ('-Fpt-', 'Fpt'), ('por', 'sps00'), ('una', 'di0fs0'), ('duración', 'ncfs000'), ('de', 'sps00'), ('25', 'Z'), ('años', 'ncmp000'), ('.', 'Fp')]\n",
            "Número total de frases en el corpus: 6030\n",
            "Número total de palabras en el corpus: 192686\n",
            "Primeras 3 frases del corpus con etiquetas reducidas:\n",
            "[('El', 'da'), ('grupo', 'nc'), ('estatal', 'aq'), ('Electricité_de_France', 'np'), ('-Fpa-', 'fpa'), ('EDF', 'np'), ('-Fpt-', 'fpt'), ('anunció', 'vmi'), ('hoy', 'rg'), (',', 'fc'), ('jueves', 'w'), (',', 'fc'), ('la', 'da'), ('compra', 'nc'), ('del', 'sp'), ('51_por_ciento', 'zp'), ('de', 'sp'), ('la', 'da'), ('empresa', 'nc'), ('mexicana', 'aq'), ('Electricidad_Águila_de_Altamira', 'np'), ('-Fpa-', 'fpa'), ('EAA', 'np'), ('-Fpt-', 'fpt'), (',', 'fc'), ('creada', 'aq'), ('por', 'sp'), ('el', 'da'), ('japonés', 'aq'), ('Mitsubishi_Corporation', 'np'), ('para', 'sp'), ('poner_en_marcha', 'vmn'), ('una', 'di'), ('central', 'nc'), ('de', 'sp'), ('gas', 'nc'), ('de', 'sp'), ('495', 'z'), ('megavatios', 'nc'), ('.', 'fp')]\n",
            "[('Una', 'di'), ('portavoz', 'nc'), ('de', 'sp'), ('EDF', 'np'), ('explicó', 'vmi'), ('a', 'sp'), ('EFE', 'np'), ('que', 'cs'), ('el', 'da'), ('proyecto', 'nc'), ('para', 'sp'), ('la', 'da'), ('construcción', 'nc'), ('de', 'sp'), ('Altamira_2', 'np'), (',', 'fc'), ('al', 'sp'), ('norte', 'nc'), ('de', 'sp'), ('Tampico', 'np'), (',', 'fc'), ('prevé', 'vmm'), ('la', 'da'), ('utilización', 'nc'), ('de', 'sp'), ('gas', 'nc'), ('natural', 'aq'), ('como', 'cs'), ('combustible', 'nc'), ('principal', 'aq'), ('en', 'sp'), ('una', 'di'), ('central', 'nc'), ('de', 'sp'), ('ciclo', 'nc'), ('combinado', 'aq'), ('que', 'pr'), ('debe', 'vmi'), ('empezar', 'vmn'), ('a', 'sp'), ('funcionar', 'vmn'), ('en', 'sp'), ('mayo_del_2002', 'w'), ('.', 'fp')]\n",
            "[('La', 'da'), ('electricidad', 'nc'), ('producida', 'aq'), ('pasará', 'vmi'), ('a', 'sp'), ('la', 'da'), ('red', 'nc'), ('eléctrica', 'aq'), ('pública', 'aq'), ('de', 'sp'), ('México', 'np'), ('en_virtud_de', 'sp'), ('un', 'di'), ('acuerdo', 'nc'), ('de', 'sp'), ('venta', 'nc'), ('de', 'sp'), ('energía', 'nc'), ('de', 'sp'), ('EAA', 'np'), ('con', 'sp'), ('la', 'da'), ('Comisión_Federal_de_Electricidad', 'np'), ('-Fpa-', 'fpa'), ('CFE', 'np'), ('-Fpt-', 'fpt'), ('por', 'sp'), ('una', 'di'), ('duración', 'nc'), ('de', 'sp'), ('25', 'z'), ('años', 'nc'), ('.', 'fp')]\n"
          ]
        }
      ],
      "source": [
        "from re import split\n",
        "#Esta linea carga el corpus en castellano, pero podemos hacerlo en ingles o catalan si os apetece\n",
        "from nltk.corpus import cess_esp\n",
        "corpus_sentences=cess_esp.tagged_sents()\n",
        "\n",
        "#Train test del corupus original\n",
        "split_index = int(len(corpus_sentences) * 0.9)\n",
        "train_data = corpus_sentences[:split_index]\n",
        "test_data = corpus_sentences[split_index:]\n",
        "\n",
        "#mostramos las 3 primeras frases del corpus (como está originalmente en el enunciado)\n",
        "print(\"Primeras 3 frases del corpus original:\")\n",
        "for sentence in corpus_sentences[:3]:\n",
        "    print(sentence)\n",
        "\n",
        "#opcional: calculamos el número de sentencias y palabras del corpus y los mostramos.\n",
        "# Número total de frases en el corpus\n",
        "num_sentences = len(corpus_sentences)\n",
        "# Número total de palabras en el corpus\n",
        "num_words = sum(len(sentence) for sentence in corpus_sentences)\n",
        "\n",
        "print(f\"Número total de frases en el corpus: {num_sentences}\")\n",
        "print(f\"Número total de palabras en el corpus: {num_words}\")\n",
        "\n",
        "\n",
        "# Escribir el corpus en un fichero\n",
        "with open('corpus.txt', 'w', encoding='utf-8') as f:\n",
        "    for sentence in corpus_sentences:\n",
        "        line = \"\\t\".join([f\"{word}\\t{tag}\" for word, tag in sentence])\n",
        "        f.write(line + \"\\n\")  # Cada oración en una línea\n",
        "\n",
        "# Leer el corpus de un fichero y guardarlo en una lista\n",
        "lista = []\n",
        "with open('corpus.txt', 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        sentence = []\n",
        "        words_tags = line.strip().split(\"\\t\")  # Separar por tabulación\n",
        "        for i in range(0, len(words_tags), 2):  # Cada palabra tiene su etiqueta al lado\n",
        "            if i + 1 < len(words_tags):\n",
        "                sentence.append((words_tags[i], words_tags[i + 1]))\n",
        "        lista.append(sentence)\n",
        "\n",
        "\n",
        "#Funcion GetLabel que reduce una etiqueta\n",
        "def getLabel(etiqueta):\n",
        "  if etiqueta.startswith(\"v\") or etiqueta.startswith(\"F\"):\n",
        "      return etiqueta[:3].lower()\n",
        "  elif len(etiqueta) >= 2:\n",
        "      return etiqueta[:2].lower()\n",
        "  else:\n",
        "      return etiqueta.lower()\n",
        "\n",
        "# 3. Eliminar tuplas vacías\n",
        "def clean_corpus(sentences):\n",
        "  cleaned_sentences = []\n",
        "  for sentence in sentences:\n",
        "      cleaned_sentence = [(word, getLabel(tag)) for word, tag in sentence if word != '0' and getLabel(tag) is not None]\n",
        "      if cleaned_sentence:\n",
        "          cleaned_sentences.append(cleaned_sentence)\n",
        "  return cleaned_sentences\n",
        "\n",
        "retic = clean_corpus(lista)\n",
        "#mostramos las 3 primeras frases del corpus con las etiquetas reducidas (como muestra el enunciado)\n",
        "print(\"Primeras 3 frases del corpus con etiquetas reducidas:\")\n",
        "for sentence in retic[:3]:\n",
        "    print(sentence)\n",
        "\n",
        "# Dividir el nuevo corpus retic en 90% entrenamiento y 10% test\n",
        "train_size = int(0.9 * len(retic))\n",
        "train_sentences = retic[:train_size]\n",
        "test_sentences = retic[train_size:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35dee72f",
      "metadata": {
        "id": "35dee72f"
      },
      "source": [
        "### b) Uso de etiquetadores morfosintácticos (hmm o tnt).\n",
        "* Entrenar los etiquetadores con la partición de train previamente construida mostrar la precisión:\n",
        "\n",
        "```\n",
        "precisión hmm: 0.8784427571832664\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5faa3b76",
      "metadata": {
        "id": "5faa3b76",
        "outputId": "83c6047f-2d7c-4c4d-b377-af925f9f44a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisión tnt: 0.8334287349742415\n",
            "Precisión hmm: 0.8895248998282771\n"
          ]
        }
      ],
      "source": [
        "#importamos el etiquetador hmm o tnt, lo entrenamos y mostramos su precisión (accuracy)\n",
        "from nltk.tag import HiddenMarkovModelTagger\n",
        "from nltk.tag import tnt\n",
        "from nltk.metrics import accuracy\n",
        "\n",
        "# Entrenar el etiquetador TNT\n",
        "tnt_tagger = tnt.TnT()\n",
        "tnt_tagger.train(train_sentences)\n",
        "\n",
        "accuracy_tnt = tnt_tagger.accuracy(test_sentences)\n",
        "\n",
        "print(\"Precisión tnt:\", accuracy_tnt)\n",
        "\n",
        "# Entrenar el etiquetador hmm\n",
        "hmm_tagger = HiddenMarkovModelTagger.train(train_sentences)\n",
        "accuracy_hmm = hmm_tagger.accuracy(test_sentences)\n",
        "\n",
        "print(\"Precisión hmm:\", accuracy_hmm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4bae1bb",
      "metadata": {
        "id": "f4bae1bb"
      },
      "source": [
        "### c) Etiquetar con dicho modelo el conjunto de test construido\n",
        "* Evaluar las prestaciones sobre el conjunto de test\n",
        "* Hacer una evaluación de las prestaciones de etiquetado usando todo el corpus (10-fold cross validation).\n",
        "\n",
        "```\n",
        "Media de la precisión de los 10 KFolds:  0.9580362988365326\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3a63aade-6345-48f4-b559-d78f275ee306",
      "metadata": {
        "id": "3a63aade-6345-48f4-b559-d78f275ee306",
        "outputId": "5f485cb7-fabf-44d5-dea3-c09ffb95c358",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.6.1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "#importamos scikit-learn y mostramos la versión para comprobar que todo funcione bien.\n",
        "import sklearn\n",
        "sklearn.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1be17685-3072-4bc9-821d-0115b69f9713",
      "metadata": {
        "scrolled": true,
        "id": "1be17685-3072-4bc9-821d-0115b69f9713",
        "outputId": "3d456442-0887-40fd-bdf3-0f5e7485378f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Media de la precisión de los 10 KFolds: 0.9094792524397667\n"
          ]
        }
      ],
      "source": [
        "#Montamos el kfold de 10 splits sin shuffle y sin random_state\n",
        "#Para cada KFold guardamos su accuracy, y finalmente mostramos la media de los 10.\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "kf = KFold(n_splits=10, shuffle=False)\n",
        "\n",
        "accuracies = []\n",
        "\n",
        "sentences = train_sentences + test_sentences\n",
        "\n",
        "for train_index, test_index in kf.split(sentences):\n",
        "    train_data = [sentences[i] for i in train_index]\n",
        "    test_data = [sentences[i] for i in test_index]\n",
        "\n",
        "    # Entrenar el modelo en cada fold\n",
        "    hmm_tagger = HiddenMarkovModelTagger.train(train_data)\n",
        "\n",
        "    # Evaluar el modelo en cada fold\n",
        "    accuracy = hmm_tagger.accuracy(test_data)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "mean_accuracy = sum(accuracies) / len(accuracies)\n",
        "print(\"Media de la precisión de los 10 KFolds:\", mean_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93b6532c-73f5-4e76-94cd-009a8f52fde0",
      "metadata": {
        "id": "93b6532c-73f5-4e76-94cd-009a8f52fde0"
      },
      "source": [
        "### d) Tokenizar una frase y etiquetarla con el modelo entrenado previamente\n",
        "* Usar word_tokenize y tag del modelo usado (tnt, hmm)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "2BsM61Xybh4D",
        "outputId": "935e602a-98cb-449f-d296-e44e0d8e8a48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2BsM61Xybh4D",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "26fab5ad-d2c8-4672-8453-f11d0739d32b",
      "metadata": {
        "id": "26fab5ad-d2c8-4672-8453-f11d0739d32b",
        "outputId": "5704a5b4-5f37-4042-a302-c25c1038c52b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Esta', 'pd'), ('es', 'vsi'), ('una', 'di'), ('frase', 'nc'), ('de', 'sp'), ('ejemplo', 'nc'), ('.', 'fp')]\n"
          ]
        }
      ],
      "source": [
        "#Usar el tokenizador de nltk\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Frase de ejemplo\n",
        "sentence = \"Esta es una frase de ejemplo.\"\n",
        "\n",
        "# Tokenizar la frase\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "# Etiquetar los tokens con el modelo entrenado\n",
        "tagged_tokens = tnt_tagger.tag(tokens)\n",
        "\n",
        "# Mostrar los resultados\n",
        "print(tagged_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44b71a91-6d42-4673-ba93-face2dddef5a",
      "metadata": {
        "id": "44b71a91-6d42-4673-ba93-face2dddef5a"
      },
      "source": [
        "### e) De manera opcional puedes probar a cambiar el idioma, el corpus y ver si afecta mucho a la precisión de los modelos."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('brown')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ju-4IC7UThVb",
        "outputId": "c1167c10-c112-4e10-a9c0-3d7bfa1797ba"
      },
      "id": "Ju-4IC7UThVb",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown\n",
        "\n",
        "# Cargar el corpus en inglés\n",
        "corpus_sentences = brown.tagged_sents()\n",
        "\n",
        "# Dividir el corpus (90% entrenamiento, 10% test)\n",
        "train_size = int(0.9 * len(corpus_sentences))\n",
        "train_sentences = corpus_sentences[:train_size]\n",
        "test_sentences = corpus_sentences[train_size:]\n",
        "\n",
        "# Entrenar y evaluar modelos (HMM y TNT)\n",
        "# Entrenar TNT\n",
        "tnt_tagger = tnt.TnT()\n",
        "tnt_tagger.train(train_sentences)\n",
        "accuracy_tnt = tnt_tagger.accuracy(test_sentences)\n",
        "print(\"Precisión TNT en inglés:\", accuracy_tnt)\n",
        "\n",
        "# Entrenar HMM\n",
        "hmm_tagger = HiddenMarkovModelTagger().train(train_sentences)\n",
        "accuracy_hmm = hmm_tagger.accuracy(test_sentences)\n",
        "print(\"Precisión HMM en inglés:\", accuracy_hmm)"
      ],
      "metadata": {
        "id": "3DAV3kINgMZu"
      },
      "id": "3DAV3kINgMZu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b7da9344-3aaa-4c68-bad1-22dad8a56974",
      "metadata": {
        "id": "b7da9344-3aaa-4c68-bad1-22dad8a56974"
      },
      "source": [
        "### f) Entrega este notebook completado (asegurate que funciona correctamente, y renombralo añadiendo tu nombre y apellidos al final) en la tarea de AULES de la unidad 3."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}